import joblib
from urduhack.normalization import normalize
from urduhack.tokenization import word_tokenizer
from urduhack.stop_words import STOP_WORDS

# Load trained model and vectorizer
model = joblib.load("plagiarism_model.pkl")
vectorizer = joblib.load("tfidf_vectorizer.pkl")

# Urdu text preprocessing function
def preprocess_urdu(text):
    normalized = normalize(text)
    tokens = normalized.split()
    # tokens = word_tokenizer(normalized)
    filtered = [token for token in tokens if token not in STOP_WORDS]
    return " ".join(filtered)

# --- ๐ INPUT Urdu text ๐ ---
input_text =  """
ูพุงฺฉุณุชุงู ฺฉุง ุชุนูู ูุธุงู ุงูุฑ ุงุณ ฺฉ ุงูุช

ูพุงฺฉุณุชุงู ุงฺฉ ุชุฑู ูพุฐุฑ ููฺฉ  ุฌุงฺบ ฺฉุง ุชุนูู ูุธุงู ูุฎุชูู ฺููุฌุฒ ฺฉุง ุณุงููุง ฺฉุฑุชุง  ุชุนูู ฺฉุณ ุจฺพ ููู ฺฉ ุชุฑู ฺฉ ู ุจุช ุงูุช ุฑฺฉฺพุช ุ ุงูุฑ ุงุณ ฺฉ ุจูุงุฏ ูพุฑ ูููฺบ ุงูพู ูุนุดุชุ ูุนุงุดุฑุชุ ุงูุฑ ุซูุงูุช ฺฉู ุจุชุฑ ุจูุงุช ฺบ ุชุงูุ ูพุงฺฉุณุชุงู ูฺบ ุชุนูู ฺฉ ุดุนุจ ูฺบ ฺฉุฆ ูุณุงุฆู ฺบ ุฌู ฺฉ ูุฌ ุณ ููฺฉ ฺฉ ุชุฑู ูฺบ ุฑฺฉุงููนฺบ ุข ุฑ ฺบ ุงู ูุณุงุฆู ูฺบ ูุนุงุฑ ฺฉ ฺฉูุ ุชุนูู ุงุฏุงุฑูฺบ ฺฉุง ููุฏุงูุ ุงูุฑ ูุณุงุฆู ฺฉ ฺฉู ุดุงูู ฺบ

ูพุงฺฉุณุชุงู ูฺบ ุชุนูู ูุธุงู ฺฉ ูุณุงุฆู ฺฉุง ุญู ูฺฉุงููุง ุงูุฑ ุงุณ ุนุงูู ูุนุงุฑ ฺฉ ูุทุงุจู ุจุชุฑ ุจูุงูุง ุถุฑูุฑ  ุงุณ ฺฉ ู ุญฺฉููุช ฺฉู ุชุนูู ุงุฏุงุฑูฺบ ฺฉ ุจุชุฑุ ุงุณุงุชุฐ ฺฉ ุชุฑุจุชุ ุงูุฑ ูุตุงุจ ูฺบ ุชุจุฏู ฺฉ ุถุฑูุฑุช  ุชุงฺฉ ุจฺูฺบ ฺฉู ุฌุฏุฏ ุงูุฑ ููุซุฑ ุชุนูู ูุฑุงู ฺฉ ุฌุง ุณฺฉ ุชุนูู ุงุฏุงุฑูฺบ ูฺบ ุจฺูฺบ ฺฉู ุตุฑู ฺฉุชุงุจ ุนูู  ูฺบุ ุจูฺฉ ุนูู ุฒูุฏฺฏ ฺฉ ู ุจฺพ ุชุงุฑ ฺฉุฑูุง ุถุฑูุฑ 

ูพุงฺฉุณุชุงู ูฺบ ุชุนูู ฺฉุง ูุนุงุฑ ูุฎุชูู ุนูุงููฺบ ูฺบ ูุฎุชูู ูุชุง  ุดุฑ ุนูุงููฺบ ูฺบ ุชุนูู ฺฉ ุณููุชฺบ ูุณุจุชุงู ุจุชุฑ ฺบุ ูฺฉู ุฏ ุนูุงููฺบ ูฺบ ุชุนูู ุงุฏุงุฑูฺบ ฺฉ ฺฉู ุงูุฑ ูุณุงุฆู ฺฉ ฺฉูุงุจ ฺฉ ูุฌ ุณ ุจฺูฺบ ฺฉู ูุนุงุฑ ุชุนูู ุญุงุตู ฺฉุฑูุง ูุดฺฉู ู ุฌุงุชุง  ุงุณ ฺฉ ุนูุงูุ ุชุนูู ุงุฏุงุฑูฺบ ูฺบ ุงุณุงุชุฐ ฺฉ ฺฉู ุงูุฑ ุงู ฺฉ ุชุฑุจุช ูฺบ ฺฉู ุจฺพ ุงฺฉ ุจฺุง ูุณุฆู 

ูพุงฺฉุณุชุงู ูฺบ ุชุนูู ฺฉ ุดุนุจ ูฺบ ุงุตูุงุญุงุช ฺฉ ุถุฑูุฑุช  ุชุงฺฉ ุฑ ุจฺ ฺฉู ูุนุงุฑ ุชุนูู ูู ุณฺฉ ุงูุฑ ู ูุนุงุดุฑุช ุชุฑู ูฺบ ุงูพูุง ฺฉุฑุฏุงุฑ ุงุฏุง ฺฉุฑ ุณฺฉ ุญฺฉููุช ฺฉู ฺุง ฺฉ ู ุชุนูู ุงุฏุงุฑูฺบ ฺฉ ุงููุฑุงุณูนุฑฺฉฺุฑ ฺฉู ุจุชุฑ ุจูุงุฆ ุงูุฑ ุงุณุงุชุฐ ฺฉ ุชุฑุจุช ูฺบ ุณุฑูุง ฺฉุงุฑ ฺฉุฑ ุงุณ ฺฉ ุนูุงูุ ุชุนูู ูุธุงู ูฺบ ุฌุฏุฏ ูนฺฉูุงููุฌ ฺฉุง ุงุณุชุนูุงู ุจฺพ ุจฺฺพุงูุง ฺุง ุชุงฺฉ ุทูุจ ุนุงูู ุณุทุญ ูพุฑ ูุณุงุจูุช ุตูุงุญุชูฺบ ฺฉ ุญุงูู ุจู ุณฺฉฺบ

ุชุนูู ฺฉ ุดุนุจ ูฺบ ุงุตูุงุญุงุช ฺฉ ู ุชูุงู ุทุจูุงุช ฺฉุง ุชุนุงูู ุถุฑูุฑ  ุญฺฉููุชุ ุงุณุงุชุฐุ ุทูุจุ ูุงูุฏู ุงูุฑ ุฏฺฏุฑ ูุนุงุดุฑุช ุงุฏุงุฑูฺบ ฺฉู ูู ฺฉุฑ ฺฉุงู ฺฉุฑูุง ูฺฏุง ุชุงฺฉ ุชุนูู ฺฉ ุดุนุจ ูฺบ ุจุชุฑ ูุงุฆ ุฌุง ุณฺฉ ุงุณ ฺฉ ุนูุงูุ ุชุนูู ุงุฏุงุฑูฺบ ูฺบ ุทูุจ ฺฉ ุฑููุงุฆ ุงูุฑ ูุดุงูุฑุช ฺฉ ูุธุงู ฺฉู ุจฺพ ุจุชุฑ ุจูุงู ฺฉ ุถุฑูุฑุช  ุชุงฺฉ ุทูุจ ฺฉู ุงูพู ุชุนูู ฺฉ ุจุงุฑ ูฺบ ุจุชุฑ ูุตู ฺฉุฑู ูฺบ ูุฏุฏ ูู ุณฺฉ

ูพุงฺฉุณุชุงู ูฺบ ุชุนูู ฺฉุง ุดุนุจ ุจุช ุงู ุ ฺฉููฺฉ ุชุนูู ฺฉ ุฐุฑุน  ู ุงูพู ูุนุงุดุฑุช ุงูุฑ ุงูุชุตุงุฏ ูุณุงุฆู ฺฉุง ุญู ุชูุงุด ฺฉุฑ ุณฺฉุช ฺบ ุงฺฏุฑ ู ุงูพู ุจฺูฺบ ฺฉู ูุนุงุฑ ุชุนูู ุฏฺบุ ุชู ู ู ุตุฑู ุงูพู ุฐุงุช ุชุฑู ฺฉ ู ุจูฺฉ ููฺฉ ฺฉ ุชุฑู ฺฉ ู ุจฺพ ุงู ฺฉุฑุฏุงุฑ ุงุฏุง ฺฉุฑ ุณฺฉุช ฺบ ุงุณ ฺฉ ู ุญฺฉููุช ฺฉู ฺุง ฺฉ ู ุชุนูู ฺฉ ุดุนุจ ูพุฑ ุฒุงุฏ ุชูุฌ ุฏ ุงูุฑ ุงุณ ฺฉ ุงูุช ฺฉู ุณูุฌฺพุช ูุฆ ุงุณ ูฺบ ุณุฑูุง ฺฉุงุฑ ฺฉุฑ

ุขุฎุฑฺฉุงุฑุ ุชุนูู ุงฺฉ ููู ฺฉ ุทุงูุช ูุช  ุงูุฑ ุฌุณ ููู ฺฉ ูพุงุณ ุชุนูู ฺฉุง ูุธุงู ูุถุจูุท ูุชุง ุ ู ูุนุงุดุฑุช ุงูุฑ ุงูุชุตุงุฏ ุชุฑู ฺฉ ู ุฒุงุฏ ฺฉุงูุงุจ ู ุณฺฉุช  ูพุงฺฉุณุชุงู ูฺบ ุชุนูู ูุธุงู ฺฉ ุงุตูุงุญุงุช ุณ ู ุตุฑู ุนูุงู ุดุนูุฑ ูฺบ ุงุถุงู ูฺฏุงุ ุจูฺฉ ููู ฺฉ ูุฌููุน ุชุฑู ุจฺพ ููฺฉู ู ุณฺฉ ฺฏ
"""


# Preprocess and vectorize
processed = preprocess_urdu(input_text)
vectorized_input = vectorizer.transform([processed])

# Predict
prediction = model.predict(vectorized_input)[0]
confidence = model.predict_proba(vectorized_input)[0][prediction]

# Result
if prediction == 1:
    print(f"โ๏ธ Plagiarized Text Detected! ({confidence*100:.2f}% confidence)")
else:
    print(f"โ Text is Original (Non-plagiarized) ({confidence*100:.2f}% confidence)")
